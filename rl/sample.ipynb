{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d8893b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4262247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch, torchrl\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torch import Tensor\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import (\n",
    "    Binary,\n",
    "    Bounded,\n",
    "    Categorical,\n",
    "    Composite,\n",
    "    LazyTensorStorage,\n",
    "    ReplayBuffer,\n",
    "    TensorSpec,\n",
    "    UnboundedContinuous\n",
    ")\n",
    "from torchrl.envs import EnvBase, SerialEnv\n",
    "from torchrl.envs.transforms import ActionMask, TransformedEnv\n",
    "from torchrl.modules import MaskedCategorical, ProbabilisticActor\n",
    "from torchrl.objectives import SoftUpdate\n",
    "from torchrl.objectives.sac import DiscreteSACLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b431a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 64 # Batch size for training\n",
    "LR = 1e-3 # Learning rate for the optimizer\n",
    "WEIGHT_DECAY = 1e-4 # Weight decay for the optimizer\n",
    "\n",
    "MAX_BOARD_SIZE = 4 # Maximum board size for the Hex game\n",
    "N_CHANNEL = 4 # Number of channels for the observation (Red, Blue, Current Player, Valid Board)\n",
    "BOARD_SIZE = 4 # Size of the Hex board (board_size x board_size)\n",
    "SWAP_RULE = True # Whether to use the swap rule in the Hex game\n",
    "\n",
    "BUFFER_SIZE = 10000 # Size of the replay buffer\n",
    "N_FRAMES_PER_BATCH = 1024 # Number of frames to store in the replay buffer per episode\n",
    "STORAGE_DEVICE = 'cpu' # Device for storing the replay buffer data\n",
    "MODEL_PARAMS = {\n",
    "    \"conv_layers\": [(32, 3), (64, 3)],\n",
    "    \"n_encoder_layers\": 2,\n",
    "    \"d_input\": N_CHANNEL,\n",
    "    \"n_heads\": 2,\n",
    "    \"d_ff\": 1024,\n",
    "    \"dropout\": 0.01,\n",
    "    \"output_flatten\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f97d6",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dfd701",
   "metadata": {},
   "source": [
    "## Base Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bfea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HexEnv(EnvBase):\n",
    "    def __init__(self, \n",
    "                 board_size: int,\n",
    "                 max_board_size: int = MAX_BOARD_SIZE,\n",
    "                 swap_rule: bool = SWAP_RULE,\n",
    "                 device: torch.device = DEVICE,\n",
    "                #  batch_size: torch.Size = torch.Size()\n",
    "                ):\n",
    "\n",
    "        # Assertions\n",
    "        assert board_size >= 1, \"Board size must be greater than or equal to 1.\"\n",
    "        assert board_size <= max_board_size, \"Board size must be less than or equal to max Board size.\"\n",
    "\n",
    "        super().__init__(device=device, spec_locked=False)\n",
    "\n",
    "        # Parameters\n",
    "        self.board_size: int = board_size\n",
    "        self.max_board_size: int = max_board_size\n",
    "        self.n_channel: int = N_CHANNEL\n",
    "        self.swap_rule: bool = swap_rule\n",
    "        # self.device: torch.device = device\n",
    "        # self.batch_size: torch.Size = batch_size # No batching at all\n",
    "\n",
    "        # Create shape variables\n",
    "        self.board_shape: torch.Size = torch.Size(\n",
    "            (self.max_board_size, self.max_board_size)\n",
    "        ) # (max_board_size, max_board_size)\n",
    "\n",
    "        # Valid board mask\n",
    "        self.valid_board: Tensor = torch.zeros(\n",
    "            self.board_shape, \n",
    "            dtype=torch.bool, \n",
    "            device=self.device\n",
    "        ) # (max_board_size, max_board_size)\n",
    "        self.valid_board[:self.board_size, :self.board_size] = 1\n",
    "\n",
    "        # Create private spec variables\n",
    "        self.observation_spec = Composite({\n",
    "            \"observation\": Binary(\n",
    "                shape=self.board_shape + (self.n_channel,),\n",
    "                # (max_board_size, max_board_size, n_channel)\n",
    "                device=self.device,\n",
    "                dtype=torch.float32\n",
    "            ),\n",
    "            \"action_mask\": Binary(\n",
    "                shape=(self.max_board_size ** 2,),\n",
    "                # (max_board_size ** 2,)\n",
    "                device=self.device,\n",
    "                dtype=torch.bool\n",
    "            )\n",
    "        })\n",
    "        self.action_spec = Categorical(\n",
    "            n=self.max_board_size ** 2,\n",
    "            # Number of discrete actions for each side of the board\n",
    "            device=self.device,\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        self.reward_spec = UnboundedContinuous(\n",
    "            shape=(1,),\n",
    "            device=device,\n",
    "            dtype=torch.float32\n",
    "        ) # Reward for both players\n",
    "\n",
    "    def _reset(self, tensordict: TensorDict | None = None, **kwargs) -> TensorDict:\n",
    "        # Initialize a fresh board\n",
    "        board: Tensor = torch.full((self.max_board_size, self.max_board_size), -1, dtype=torch.long, device=self.device) # -1: empty, 0: player 0 (red), 1: player 1 (blue)\n",
    "        current_player: int = 0 # 0: player 0 (red), 1: player 1 (blue)\n",
    "        # valid_move: Tensor = self.valid_board.float() # All valid moves at the start\n",
    "        done: Tensor = torch.tensor(False, dtype=torch.bool, device=self.device) # Game not done\n",
    "        # reward: Tensor = torch.tensor([0.0], dtype=torch.float32, device=self.device) # No reward at the start\n",
    "\n",
    "        # Create fresh observation, mask, done, reward\n",
    "        fresh_action: Tensor = torch.tensor([0], dtype=torch.long, device=self.device) # Placeholder action\n",
    "        fresh_observation: Tensor = torch.zeros((self.max_board_size, self.max_board_size, self.n_channel), dtype=torch.float32, device=self.device) # (max_board_size, max_board_size, n_channel)\n",
    "        fresh_observation[..., 0] = (board == 0).float() # Red pieces channel\n",
    "        fresh_observation[..., 1] = (board == 1).float() # Blue pieces channel\n",
    "        fresh_observation[..., -2] = current_player # 0: player 0 (red), 1: player 1 (blue)\n",
    "        fresh_observation[..., -1] = self.valid_board.clone().float() # (max_board_size, max_board_size) Playable board mask\n",
    "        fresh_action_mask: Tensor = self.valid_board.clone().bool().flatten() # (max_board_size ** 2,) Valid move mask\n",
    "        fresh_done: Tensor = done # Not done\n",
    "        # fresh_reward: Tensor = reward # No reward at the start\n",
    "\n",
    "        # # Update action spec for the environment\n",
    "        # self.action_spec.update_mask(fresh_action_mask.flatten())\n",
    "\n",
    "        fresh_tensordict: TensorDict = TensorDict({\n",
    "            \"action\": fresh_action,\n",
    "            \"observation\": fresh_observation,\n",
    "            \"action_mask\": fresh_action_mask,\n",
    "            \"done\": fresh_done,\n",
    "            # \"reward\": fresh_reward\n",
    "        }, device=self.device)\n",
    "        # # Update tensordict\n",
    "        # if not isinstance(tensordict, TensorDict):\n",
    "        #     fresh_tensordict = TensorDict({\n",
    "        #         \"action\": fresh_action,\n",
    "        #         \"observation\": fresh_observation,\n",
    "        #         \"action_mask\": fresh_action_mask,\n",
    "        #         \"done\": fresh_done,\n",
    "        #         # \"reward\": fresh_reward\n",
    "        #     }, device=self.device)\n",
    "        # else:\n",
    "        #     fresh_tensordict: TensorDict = tensordict\n",
    "        #     fresh_tensordict.update({\n",
    "        #         \"action\": fresh_action,\n",
    "        #         \"observation\": fresh_observation,\n",
    "        #         \"action_mask\": fresh_action_mask,\n",
    "        #         \"done\": fresh_done,\n",
    "        #         # \"reward\": fresh_reward\n",
    "        #     })\n",
    "\n",
    "        return fresh_tensordict\n",
    "\n",
    "    def _step(self, tensordict: TensorDict, **kwargs) -> TensorDict:\n",
    "        # Extract action\n",
    "        action: Tensor = tensordict.get(\"action\").clone() # Scalar tensor representing the action\n",
    "        observation: Tensor = tensordict.get(\"observation\").clone() # (max_board_size, max_board_size, n_channel)\n",
    "        action_mask: Tensor = tensordict.get(\"action_mask\").clone() # (max_board_size ** 2,)\n",
    "        done: Tensor = tensordict.get(\"done\").clone() # Scalar tensor representing if the game is done\n",
    "        reward: Tensor = self.reward_spec.zero() # Initialize reward tensor # (1,)\n",
    "        # reward: Tensor = tensordict.get(\"reward\").clone() # (2,)\n",
    "\n",
    "        # Extract indexes of action from observation\n",
    "        index: int = int(action.item())\n",
    "        row, col = divmod(index, self.max_board_size) # Convert flat index to 2D coordinates\n",
    "\n",
    "        # Extract current state from observation\n",
    "        current_player: int = int(observation[0, 0, -2].item()) # 0: player 0 (red), 1: player 1 (blue)\n",
    "\n",
    "        # Check if this is a swap situation\n",
    "        is_first_move = (torch.sum(observation[..., 0:2]).item() == 0 and\n",
    "                         current_player == 0)  # Player 0's turn and no pieces placed yet\n",
    "        is_second_move = (torch.sum(observation[..., 0:2]).item() == 1 and\n",
    "                          current_player == 1)  # Player 1's turn and only one piece placed\n",
    "        is_swap_action = (self.swap_rule and\n",
    "                        is_second_move and\n",
    "                        observation[row, col, 0] == 1) # Player 1 selecting player 0's piece\n",
    "\n",
    "        # Validate action\n",
    "        is_valid = (\n",
    "            0 <= row < self.max_board_size and # Must be within board's max bounds\n",
    "            0 <= col < self.max_board_size and # Must be within board's max bounds\n",
    "            self.valid_board[row, col] and # Must be in valid board area\n",
    "            (action_mask[index] == 1 or is_swap_action)  # Must be empty to place a piece, or a valid swap action\n",
    "        )\n",
    "\n",
    "        # If action is not valid (only when action_spec mask is not working properly)\n",
    "        if not is_valid:\n",
    "            raise ValueError(f\"Invalid action {action.item()} at row={row}; col={col}; valid={self.valid_board[row, col]}, action_mask={action_mask[index]}.\")\n",
    "            # reward[self.current_player - 1] = -1.0 # Penalty for invalid move\n",
    "            # self.done = False # Continue the game even if the move is invalid\n",
    "            # new_observation, new_action_mask = tensordict.get(\"observation\"), tensordict.get(\"action_mask\") # Keep previous observation and action_mask\n",
    "        else:\n",
    "            # Update action_mask to prevent placing another piece here\n",
    "            action_mask[index] = 0 # Update action_mask to prevent placing another piece here\n",
    "\n",
    "            # Place piece or swap\n",
    "            if is_swap_action: # Swap the pieces\n",
    "                observation[..., 0], observation[..., 1] = observation[..., 1].clone(), observation[..., 0].clone()\n",
    "            else: # Place the piece on the board\n",
    "                observation[row, col, current_player] = 1.0 # Update observation for the current player\n",
    "\n",
    "            # Check for win condition (placeholder logic)\n",
    "            if self._check_done(observation, current_player):\n",
    "                reward: Tensor = torch.tensor([1.0 * (1 - current_player) - 1.0 * current_player], dtype=torch.float32, device=self.device) # Single reward for the current player (+1 if player 0 wins, -1 if player 1 wins)\n",
    "                done = torch.tensor(True, dtype=torch.bool) # Game done\n",
    "            else:\n",
    "                reward: Tensor = torch.tensor([0.0], dtype=torch.float32, device=self.device) # Initialize reward\n",
    "                done = torch.tensor(False, dtype=torch.bool) # Game not done\n",
    "\n",
    "                # Switch player\n",
    "                current_player = 1 - current_player # Switch between 0 and 1\n",
    "\n",
    "            # Update observation, action_mask\n",
    "            new_observation: Tensor = torch.zeros((self.max_board_size, self.max_board_size, self.n_channel), dtype=torch.float, device=self.device) # (max_board_size, max_board_size, n_channel)\n",
    "            new_observation[..., 0] = observation[..., 0] # Red pieces channel\n",
    "            new_observation[..., 1] = observation[..., 1] # Blue pieces channel\n",
    "            new_observation[..., -2] = float(current_player) # Current player channel\n",
    "            new_observation[..., -1] = observation[..., -1] # (max_board_size, max_board_size) Playable board action_mask (doesn't change)\n",
    "            new_action_mask: Tensor = action_mask.bool() # Valid move action_mask\n",
    "\n",
    "        # Create done, reward tensors\n",
    "        new_action: Tensor = action\n",
    "        new_done: Tensor = done\n",
    "        new_reward: Tensor = reward\n",
    "\n",
    "        # # Update action spec for the environment\n",
    "        # if is_first_move and self.swap_rule:\n",
    "        #     # Allow swap action if it's the first move and swap rule is enabled\n",
    "        #     swap_action_mask = new_action_mask.clone()\n",
    "        #     swap_action_mask[index] = 1 # Allow the swap action\n",
    "        #     self.action_spec.update_action_mask(swap_action_mask.flatten())\n",
    "        # else:\n",
    "        #     # Update action spec for the environment\n",
    "        #     self.action_spec.update_action_mask(new_action_mask.flatten())\n",
    "\n",
    "        # Update tensordict\n",
    "        new_tensordict = TensorDict({\n",
    "            \"action\": new_action,\n",
    "            \"observation\": new_observation,\n",
    "            \"action_mask\": new_action_mask,\n",
    "            \"done\": new_done,\n",
    "            \"reward\": new_reward\n",
    "        }, device=self.device)\n",
    "\n",
    "        return new_tensordict\n",
    "\n",
    "    def _check_done(self, observation: Tensor, current_player: int) -> bool:\n",
    "        def dfs(board, start_positions, target_condition, directions):\n",
    "            visited = torch.zeros((self.board_size, self.board_size), dtype=torch.bool)\n",
    "            for start in start_positions:\n",
    "                if board[start] == 1 and not visited[start]:\n",
    "                    stack = [start]\n",
    "                    visited[start] = True\n",
    "                    while stack:\n",
    "                        r, c = stack.pop()\n",
    "                        if target_condition(r, c):\n",
    "                            return True\n",
    "                        for dr, dc in directions:\n",
    "                            nr, nc = r + dr, c + dc\n",
    "                            if 0 <= nr < self.board_size and 0 <= nc < self.board_size and board[nr, nc] == 1 and not visited[nr, nc]:\n",
    "                                visited[nr, nc] = True\n",
    "                                stack.append((nr, nc))\n",
    "            return False\n",
    "\n",
    "        directions = [(-1,0), (1,0), (0,-1), (0,1), (1,-1), (-1,1)] # 6 possible directions in a hex grid\n",
    "        board_state = observation[:self.board_size, :self.board_size, :]\n",
    "        # Use DFS to check if player 0 (red) has connected top to bottom\n",
    "        if current_player == 0:\n",
    "            board = board_state[..., 0] # Shape (board_size, board_size) # Player 0 pieces\n",
    "            start_positions = [(0, col) for col in range(self.board_size)]\n",
    "            target_condition = lambda r, c: r == self.board_size - 1\n",
    "            if dfs(board, start_positions, target_condition, directions):\n",
    "                return True\n",
    "\n",
    "        # Use DFS to check if player 1 (blue) has connected left to right\n",
    "        else:\n",
    "            board = board_state[..., 1] # Shape (board_size, board_size) # Player 1 pieces\n",
    "            start_positions = [(row, 0) for row in range(self.board_size)]\n",
    "            target_condition = lambda r, c: c == self.board_size - 1\n",
    "            if dfs(board, start_positions, target_condition, directions):\n",
    "                return True\n",
    "\n",
    "        return False # No winner yet\n",
    "\n",
    "    def _set_seed(self, seed: int) -> None:\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0906b725",
   "metadata": {},
   "source": [
    "## Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec1a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_hex_env = lambda: HexEnv(board_size=BOARD_SIZE, max_board_size=MAX_BOARD_SIZE, device=STORAGE_DEVICE)\n",
    "serial_env = TransformedEnv(\n",
    "    SerialEnv(\n",
    "        num_workers=1,\n",
    "        create_env_fn=create_hex_env\n",
    "    ),\n",
    "    ActionMask()\n",
    ")\n",
    "# serial_env = TransformedEnv(\n",
    "#     HexEnv(\n",
    "#         board_size=BOARD_SIZE,\n",
    "#         max_board_size=MAX_BOARD_SIZE,\n",
    "#         device=STORAGE_DEVICE\n",
    "#     ),\n",
    "#     ActionMask()\n",
    "# )\n",
    "\n",
    "r = serial_env.rollout(100)\n",
    "r[\"action\"].to(dtype=torch.int)\n",
    "serial_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f822f2b6",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4739e97",
   "metadata": {},
   "source": [
    "## Custom Sub-modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2bb610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HexConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size: int = 3, bias: bool = True):\n",
    "        super(HexConv2d, self).__init__()\n",
    "        assert kernel_size % 2 == 1 and kernel_size > 0, \"kernel_size must be odd and positive.\"\n",
    "\n",
    "        stride, padding = 1, kernel_size // 2  # To maintain spatial dimensions\n",
    "        mask = self._create_hex_mask(kernel_size)\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n",
    "        self.register_buffer('mask', mask) # (k, k), requires_grad=False to keep it fixed\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_hex_mask(kernel_size: int) -> Tensor:\n",
    "        assert kernel_size % 2 == 1 and kernel_size > 0, \"kernel_size must be odd and positive.\"\n",
    "\n",
    "        mask = torch.zeros((kernel_size, kernel_size), dtype=torch.float32)\n",
    "        center = kernel_size // 2\n",
    "\n",
    "        for r in range(kernel_size): # Row index\n",
    "            for c in range(kernel_size): # Column index\n",
    "                # Using axial distance for a vertically oriented hex grid\n",
    "                # mapped to an offset coordinate system in the kernel.\n",
    "                # (r, c) are kernel indices, (dr, dc) are relative to center.\n",
    "                dr, dc = r - center, c - center\n",
    "                chebyshev_distance = max(abs(dr), abs(dc), abs(dr + dc))\n",
    "\n",
    "                if chebyshev_distance <= center: # Inside or on the hexagon\n",
    "                    mask[r, c] = 1.0\n",
    "\n",
    "        return mask # (kernel_size, kernel_size)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # 1. Tạo trọng số đã mask (Soft masking)\n",
    "        # Phép nhân '*' tạo ra tensor mới, KHÔNG sửa in-place trọng số gốc.\n",
    "        # Gradient vẫn truyền ngược qua đây bình thường, các vị trí mask=0 sẽ có grad=0.\n",
    "        masked_weight = self.conv.weight * self.mask\n",
    "        \n",
    "        # 2. Dùng functional conv2d thay vì self.conv(x)\n",
    "        # Chúng ta truyền masked_weight vào đây.\n",
    "        x = F.conv2d(\n",
    "            input=x,\n",
    "            weight=masked_weight,\n",
    "            bias=self.conv.bias,\n",
    "            stride=self.conv.stride,\n",
    "            padding=self.conv.padding,\n",
    "            dilation=self.conv.dilation,\n",
    "            groups=self.conv.groups\n",
    "        )\n",
    "        return x\n",
    "\n",
    "\n",
    "class SkipConnection(nn.Module):\n",
    "    def __init__(self, adjust_input: nn.Module, original_input: nn.Module = nn.Identity()):\n",
    "        super(SkipConnection, self).__init__()\n",
    "        self.adjust_input = adjust_input\n",
    "        self.original_input = original_input\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.adjust_input(x) + self.original_input(x)\n",
    "\n",
    "\n",
    "class TriAxialPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Kích thước channel của input (C). Phải là số chẵn.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % 2 == 0, \"d_model (C) phải là số chẵn để tính sin/cos.\"\n",
    "\n",
    "        # Pre-compute div_term cho sinusoidal\n",
    "        # Lưu ý: arange(0, d_model, 2) tạo ra d_model/2 phần tử\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.register_buffer('div_term', div_term)\n",
    "\n",
    "    def _get_1d_sinusoidal(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Tạo sinusoidal embedding cho một trục toạ độ.\n",
    "        Args:\n",
    "            coords: Tensor chứa giá trị toạ độ (số nguyên hoặc thực), shape (H, W)\n",
    "        Returns:\n",
    "            Tensor embedding, shape (H, W, d_model)\n",
    "        \"\"\"\n",
    "        # Mở rộng chiều cuối để tính toán: (H, W, 1)\n",
    "        coords = coords.unsqueeze(-1).float()\n",
    "        \n",
    "        # div_term có shape (d_model/2,)\n",
    "        # phase = coords * div_term -> Shape (H, W, d_model/2)\n",
    "        phase = coords * self.div_term\n",
    "        \n",
    "        # Tính sin và cos riêng biệt\n",
    "        sin_part = torch.sin(phase) # (H, W, d_model/2)\n",
    "        cos_part = torch.cos(phase) # (H, W, d_model/2)\n",
    "        \n",
    "        # --- FIX LỖI VMAP Ở ĐÂY ---\n",
    "        # Thay vì gán in-place (pe[..., 0::2] = ...), ta dùng stack và flatten.\n",
    "        # 1. Stack lại ở chiều cuối cùng: (H, W, d_model/2, 2)\n",
    "        #    Tại vị trí cuối: [sin, cos], [sin, cos], ...\n",
    "        val = torch.stack([sin_part, cos_part], dim=-1)\n",
    "        \n",
    "        # 2. Flatten 2 chiều cuối để trộn lại thành (H, W, d_model)\n",
    "        #    Kết quả sẽ là: sin, cos, sin, cos... đúng thứ tự chẵn lẻ\n",
    "        pe = val.flatten(-2, -1)\n",
    "        \n",
    "        return pe\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # ... (Giữ nguyên phần logic forward cũ) ...\n",
    "        N, H, W, C = x.shape\n",
    "        assert C == self.d_model, f\"Input channel {C} không khớp với d_model khởi tạo {self.d_model}\"\n",
    "\n",
    "        device = x.device\n",
    "\n",
    "        rows = torch.arange(H, device=device, dtype=torch.float)\n",
    "        cols = torch.arange(W, device=device, dtype=torch.float)\n",
    "        \n",
    "        r_grid, q_grid = torch.meshgrid(rows, cols, indexing='ij')\n",
    "\n",
    "        s_grid = -q_grid - r_grid\n",
    "\n",
    "        pe_q = self._get_1d_sinusoidal(q_grid)\n",
    "        pe_r = self._get_1d_sinusoidal(r_grid)\n",
    "        pe_s = self._get_1d_sinusoidal(s_grid)\n",
    "\n",
    "        full_pe = (pe_q + pe_r + pe_s) / math.sqrt(3)\n",
    "\n",
    "        return full_pe.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852c3b06",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc416374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HexModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 conv_layers: list[tuple[int, int]],\n",
    "                 n_encoder_layers: int,\n",
    "                 d_input: int,\n",
    "                 n_heads: int = 8,\n",
    "                 d_ff: int = 2048,\n",
    "                 dropout: float = 0.1,\n",
    "                 output_flatten: bool = True):\n",
    "        \"\"\"Args:\n",
    "            conv_layers: List of tuples (out_channels, kernel_size) for each conv layer.\n",
    "                Note that, in_channels is inferred from the previous layer's out_channels (d_input for the first layer).\n",
    "            n_encoder_layers: Number of transformer encoder layers.\n",
    "            d_input: Dimension of input features to the transformer.\n",
    "            n_heads: Number of attention heads in the transformer.\n",
    "            d_ff: Dimension of the feedforward network in the transformer.\n",
    "            dropout: Dropout rate.\n",
    "        \"\"\"\n",
    "        super(HexModel, self).__init__()\n",
    "        self.output_flatten = output_flatten\n",
    "        self.d_encoder: int = conv_layers[-1][0] # Last conv layer's out_channels as d_model\n",
    "        self.conv = nn.Sequential(*[\n",
    "            SkipConnection(\n",
    "                nn.Sequential(\n",
    "                    HexConv2d(conv_layers[i-1][0] if i > 0 else d_input, conv_layers[i][0], conv_layers[i][1], bias=False),\n",
    "                    nn.GroupNorm(num_groups=4, num_channels=conv_layers[i][0]),\n",
    "                    nn.GELU(),\n",
    "                    HexConv2d(conv_layers[i][0], conv_layers[i][0], conv_layers[i][1], bias=False),\n",
    "                    nn.GroupNorm(num_groups=4, num_channels=conv_layers[i][0]),\n",
    "                ),\n",
    "                nn.Identity() if conv_layers[i][0] == conv_layers[i-1][0] # Skip connection (identity)\n",
    "                else nn.Sequential(\n",
    "                    nn.Conv2d(conv_layers[i-1][0] if i > 0 else d_input, conv_layers[i][0], 1), # Combine with Conv2d (kernel_size = 1) for channel adjustment\n",
    "                    nn.GroupNorm(num_groups=4, num_channels=conv_layers[i][0]),\n",
    "                )\n",
    "            )\n",
    "            for i in range(len(conv_layers))\n",
    "        ])\n",
    "        self.positional_embedding = TriAxialPositionalEmbedding(self.d_encoder)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=self.d_encoder,\n",
    "                nhead=n_heads,\n",
    "                dim_feedforward=d_ff,\n",
    "                dropout=dropout,\n",
    "                activation='gelu',\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=n_encoder_layers\n",
    "        )\n",
    "        self.projection = nn.Linear(self.d_encoder, 1) # Đầu ra cho Actor (logits)/Critic (Q-value)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Args:\n",
    "            x: Input tensor of shape (N, H, W, C) where\n",
    "               N = batch size, H = height, W = width, C = channels (d_input).\n",
    "            Returns: Tensor of shape (N, H, W) with Q-values for each position.\n",
    "        \"\"\"\n",
    "        # Reshape input to (N, C, H, W) for Conv2d\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(0)  # Add batch dimension if missing\n",
    "        elif len(x.shape) != 4:\n",
    "            raise ValueError(f\"Input tensor x must have shape (N, H, W, C) or (H, W, C), but got {x.shape}.\")\n",
    "\n",
    "        # 1. Convolutional layers\n",
    "        batch_size, height, width = x.size(0), x.size(1), x.size(2)\n",
    "        x = x.permute(0, 3, 1, 2).contiguous() # (N, C, H, W)\n",
    "        x = self.conv(x) # (N, d_encoder, H, W)\n",
    "\n",
    "        # 2. Positional Embedding + Transformer Encoder\n",
    "        # x = x.permute(0, 2, 3, 1).flatten(1, 2).contiguous()\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        pe: Tensor = self.positional_embedding(x)\n",
    "        x = (x + pe).flatten(1, 2).contiguous() # (N, H*W, d_encoder)\n",
    "        x = self.encoder(x) # (N, H*W, d_encoder)\n",
    "\n",
    "        # Chỉ sử dụng khi sử dụng vmap của DiscreteSACLOss (deactivate_vmap=False)\n",
    "        # Nếu không dùng vmap thì không cần thiết (do giảm hiệu suất).\n",
    "        # with sdpa_kernel(SDPBackend.MATH):\n",
    "        #     x = self.encoder(x) # (N, H*W, d_encoder)\n",
    "\n",
    "        # 3. Projection to create outputs for Actor/Critic\n",
    "        x = self.projection(x) # (N, H*W, 1)\n",
    "        if self.output_flatten:\n",
    "            return x.view(batch_size, -1) # (N, H*W)\n",
    "        else:\n",
    "            return x.view(batch_size, height, width) # (N, H, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f307f84e",
   "metadata": {},
   "source": [
    "## Policy Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731454aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorWrapper(nn.Module):\n",
    "    \"\"\"Bọc TransformerQL_AC, chỉ trả về 'logits'.\"\"\"\n",
    "    def __init__(self, model: HexModel):\n",
    "        super().__init__()\n",
    "        self.model = model # Tham chiếu đến model chung\n",
    "\n",
    "    def forward(self, observation: Tensor, action_mask: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        action_mask = action_mask.view(observation.shape[0], -1) # (N, H*W)\n",
    "        \n",
    "        # Chạy model chung, chỉ lấy đầu ra đầu tiên\n",
    "        logits = self.model(observation) # logits shape (N, H*W)\n",
    "        \n",
    "        # logits[~action_mask] = -torch.inf # Áp dụng mask\n",
    "        return logits, action_mask\n",
    "\n",
    "\n",
    "class CriticWrapper(nn.Module):\n",
    "    \"\"\"Bọc HexModel, chỉ trả về 'action_value'.\"\"\"\n",
    "    def __init__(self, model: HexModel):\n",
    "        super().__init__()\n",
    "        self.model = model # Tham chiếu đến CÙNG model chung\n",
    "\n",
    "    def forward(self, observation: Tensor, action_mask: Tensor) -> Tensor:\n",
    "        # action_mask = action_mask.view(observation.shape[0], -1)\n",
    "        \n",
    "        # Chạy model chung, chỉ lấy đầu ra thứ hai\n",
    "        q_values = self.model(observation) # q_values shape (N, H*W)\n",
    "        \n",
    "        # q_values[~action_mask] = -torch.inf # Áp dụng mask\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdddfd46",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Hàm khởi tạo tham số tối ưu\n",
    "def init_params(model: nn.Module):\n",
    "    \"\"\"\n",
    "    Khởi tạo tham số (Weights Initialization) tối ưu cho RL & GELU.\n",
    "    \"\"\"\n",
    "    for m in model.modules():\n",
    "        # 1. Xử lý Linear và Conv2d (bao gồm cả trong HexConv2d)\n",
    "        if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "            # Nếu là lớp Projection cuối cùng: Init nhỏ để Policy bắt đầu ngẫu nhiên (Max Entropy)\n",
    "            if hasattr(model, 'projection') and m is model.projection:\n",
    "                nn.init.orthogonal_(m.weight, gain=0.01)\n",
    "            # Các lớp ẩn: Gain = sqrt(2) phù hợp với GELU/ReLU\n",
    "            else:\n",
    "                nn.init.orthogonal_(m.weight, gain=math.sqrt(2))\n",
    "            \n",
    "            # Luôn đưa bias về 0\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "        \n",
    "        # 2. Xử lý Normalization (GroupNorm, LayerNorm)\n",
    "        elif isinstance(m, (nn.GroupNorm, nn.LayerNorm, nn.BatchNorm2d)):\n",
    "            if m.weight is not None:\n",
    "                nn.init.constant_(m.weight, 1.0) # Gamma = 1\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0.0)   # Beta = 0\n",
    "\n",
    "def get_optimizer_params(model: nn.Module, weight_decay: float = 1e-5):\n",
    "    \"\"\"\n",
    "    Tạo dictionary tham số cho AdamW, tách biệt nhóm cần decay và nhóm không.\n",
    "    Args:\n",
    "        weight_decay: Hệ số weight decay mong muốn.\n",
    "    Returns:\n",
    "        List các dict config cho optimizer.\n",
    "    \"\"\"\n",
    "    decay_params = []\n",
    "    no_decay_params = []\n",
    "    \n",
    "    # Danh sách các lớp mà weight của nó CẦN decay\n",
    "    whitelist_weight_modules = (nn.Linear, nn.Conv2d)\n",
    "    # Danh sách các lớp mà weight của nó KHÔNG decay (Norm layers)\n",
    "    blacklist_weight_modules = (nn.GroupNorm, nn.LayerNorm, nn.BatchNorm2d)\n",
    "\n",
    "    # Duyệt qua tất cả module con\n",
    "    for mn, m in model.named_modules():\n",
    "        for pn, p in m.named_parameters(recurse=False):\n",
    "            # Chỉ xét tham số cần học (Mask và Buffer đã tự động bị loại vì requires_grad=False)\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            \n",
    "            full_param_name = f\"{mn}.{pn}\" if mn else pn\n",
    "\n",
    "            # 1. Tất cả Bias -> KHÔNG decay\n",
    "            if pn.endswith('bias'):\n",
    "                no_decay_params.append(p)\n",
    "            \n",
    "            # 2. Weight của Conv/Linear -> CÓ decay\n",
    "            elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                decay_params.append(p)\n",
    "            \n",
    "            # 3. Weight của Norm (Gamma) -> KHÔNG decay\n",
    "            elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                no_decay_params.append(p)\n",
    "            \n",
    "            # 4. Các trường hợp khác (nếu có) -> Mặc định KHÔNG decay cho an toàn\n",
    "            else:\n",
    "                no_decay_params.append(p)\n",
    "\n",
    "    # Kiểm tra nhanh để đảm bảo không bỏ sót tham số nào\n",
    "    param_dict = {pn: p for pn, p in model.named_parameters() if p.requires_grad}\n",
    "    inter_params = len(decay_params) + len(no_decay_params)\n",
    "    assert len(param_dict.keys()) == inter_params, f\"Lỗi: Tổng tham số lọc được ({inter_params}) không khớp với model ({len(param_dict.keys())})\"\n",
    "\n",
    "    return [\n",
    "        {'params': decay_params, 'weight_decay': weight_decay},\n",
    "        {'params': no_decay_params, 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "def merge_optimizer_params(loss_fn_params, actor_groups: list, qvalue_groups: list):\n",
    "    \"\"\"\n",
    "    Gộp các nhóm tham số lại với nhau, loại bỏ trùng lặp theo thứ tự ưu tiên.\n",
    "    \n",
    "    Priority:\n",
    "    1. Actor Groups (Custom weight decay logic)\n",
    "    2. QValue Groups (Custom weight decay logic)\n",
    "    3. Loss Module Leftovers (Ví dụ: log_alpha trong SAC, thường không có weight decay)\n",
    "    \n",
    "    Args:\n",
    "        loss_fn_params: Iterable (thường là loss_fn.parameters())\n",
    "        actor_groups: List dicts từ actor_model.get_optimizer_params()\n",
    "        qvalue_groups: List dicts từ qvalue_model.get_optimizer_params()\n",
    "        \n",
    "    Returns:\n",
    "        List các dict config cho optimizer.\n",
    "    \"\"\"\n",
    "    final_groups = []\n",
    "    seen_param_ids = set()\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 1. Ưu tiên cao nhất: Actor Model\n",
    "    # -------------------------------------------------------\n",
    "    for group in actor_groups:\n",
    "        # Lọc những param đã tồn tại (trường hợp hiếm nếu actor/critic share weights)\n",
    "        params = group['params']\n",
    "        new_params = []\n",
    "        for p in params:\n",
    "            if id(p) not in seen_param_ids:\n",
    "                seen_param_ids.add(id(p))\n",
    "                new_params.append(p)\n",
    "        \n",
    "        if new_params:\n",
    "            # Tạo bản copy của group để không sửa đổi input gốc\n",
    "            new_group = group.copy()\n",
    "            new_group['params'] = new_params\n",
    "            final_groups.append(new_group)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 2. Ưu tiên nhì: QValue Model\n",
    "    # -------------------------------------------------------\n",
    "    for group in qvalue_groups:\n",
    "        params = group['params']\n",
    "        new_params = []\n",
    "        for p in params:\n",
    "            # Chỉ lấy tham số chưa xuất hiện trong Actor\n",
    "            if id(p) not in seen_param_ids:\n",
    "                seen_param_ids.add(id(p))\n",
    "                new_params.append(p)\n",
    "        \n",
    "        if new_params:\n",
    "            new_group = group.copy()\n",
    "            new_group['params'] = new_params\n",
    "            final_groups.append(new_group)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 3. Ưu tiên thấp nhất: Loss Module (Leftovers)\n",
    "    # (Nơi chứa log_alpha hoặc các tham số tự động của TorchRL)\n",
    "    # -------------------------------------------------------\n",
    "    leftover_params = []\n",
    "    for p in loss_fn_params:\n",
    "        if id(p) not in seen_param_ids:\n",
    "            leftover_params.append(p)\n",
    "            seen_param_ids.add(id(p))\n",
    "    \n",
    "    if leftover_params:\n",
    "        # Các tham số \"thừa\" này thường là hệ số học (như alpha), \n",
    "        # không nên áp dụng weight decay cho chúng.\n",
    "        final_groups.append({\n",
    "            'params': leftover_params, \n",
    "            'weight_decay': 0.0 \n",
    "        })\n",
    "\n",
    "    return final_groups\n",
    "\n",
    "def check_params_changed(model, model_name, old_params):\n",
    "    changed = False\n",
    "    print(f\"Checking updates for {model_name}...\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            # Lấy tham số cũ tương ứng\n",
    "            old_p = old_params[name]\n",
    "            # Tính sự khác biệt (norm của hiệu)\n",
    "            diff = (param - old_p).abs().sum().item()\n",
    "            \n",
    "            if diff > 0:\n",
    "                changed = True\n",
    "                # Chỉ in ra một vài layer đại diện để không làm rối màn hình\n",
    "                if \"weight\" in name and diff > 1e-6: \n",
    "                    print(f\"  ✓ {name} changed (diff: {diff:.6f})\")\n",
    "            else:\n",
    "                 print(f\"  ⚠️ {name} did NOT change (diff: 0.0)\")\n",
    "    \n",
    "    if changed:\n",
    "        print(f\"✅ {model_name} weights updated successfully.\")\n",
    "    else:\n",
    "        print(f\"❌ {model_name} weights did NOT update. Check gradients or learning rate.\")\n",
    "    return changed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1718210c",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5986ee8",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40981d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tạo model gốc\n",
    "actor_model = HexModel(**MODEL_PARAMS).train().to(DEVICE) # Model cho actor\n",
    "qvalue_model = HexModel(**MODEL_PARAMS).train().to(DEVICE) # Model cho critic\n",
    "init_params(actor_model)\n",
    "init_params(qvalue_model)\n",
    "# model = HexModel(**MODEL_PARAMS).train().to(DEVICE) # Dùng chung cho cả actor và critic\n",
    "# init_params(model)\n",
    "# actor_model, qvalue_model = model, model\n",
    "\n",
    "# 2. Tạo hai wrapper và policy\n",
    "actor_network = TensorDictModule(\n",
    "    ActorWrapper(actor_model),\n",
    "    in_keys=[\"observation\", \"action_mask\"],\n",
    "    out_keys=[\"logits\", \"mask\"]\n",
    ")\n",
    "qvalue_network = TensorDictModule(\n",
    "    CriticWrapper(qvalue_model),\n",
    "    in_keys=[\"observation\", \"action_mask\"],\n",
    "    out_keys=[\"action_value\"]\n",
    ")\n",
    "actor = ProbabilisticActor(\n",
    "    actor_network,\n",
    "    in_keys=[\"logits\", \"mask\"],\n",
    "    spec=serial_env.action_spec,\n",
    "    distribution_class=MaskedCategorical\n",
    ")\n",
    "\n",
    "# 3. Tạo loss_fn, optimizer, updater\n",
    "loss_fn = DiscreteSACLoss(\n",
    "    actor_network=actor,\n",
    "    qvalue_network=qvalue_network,\n",
    "    action_space=serial_env.action_spec,\n",
    "    num_actions=serial_env.action_spec.n,\n",
    "    deactivate_vmap=True\n",
    "    # Do Transformer không hỗ trợ vmap tốt. Nếu muốn dùng vmap, cần sử dụng SDPBackend.MATH\n",
    ").to(DEVICE)\n",
    "\n",
    "## Lấy các nhóm tham số ưu tiên từ Actor và Critic (đã có config weight_decay chuẩn)\n",
    "actor_params_groups = get_optimizer_params(actor_model, WEIGHT_DECAY)\n",
    "qvalue_params_groups = get_optimizer_params(qvalue_model, WEIGHT_DECAY)\n",
    "\n",
    "## Gộp các nhóm tham số lại, ưu tiên Actor > Critic > Loss leftovers\n",
    "combined_params = merge_optimizer_params(\n",
    "    loss_fn_params=loss_fn.parameters(),\n",
    "    actor_groups=actor_params_groups,\n",
    "    qvalue_groups=qvalue_params_groups\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=combined_params,\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY \n",
    ")\n",
    "updater = SoftUpdate(\n",
    "    loss_module=loss_fn,\n",
    "    tau=0.005\n",
    ")\n",
    "\n",
    "# 4. Thiết lập optimizer, replay buffer, và các thành phần khác như bình thường\n",
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(BUFFER_SIZE, device=STORAGE_DEVICE),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "collector = SyncDataCollector(\n",
    "    create_env_fn=serial_env,\n",
    "    policy=actor,\n",
    "    frames_per_batch=N_FRAMES_PER_BATCH,\n",
    "    total_frames=-1, # Vô hạn\n",
    "    device=DEVICE,\n",
    "    storing_device=STORAGE_DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9c9cf1",
   "metadata": {},
   "source": [
    "## Functionality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f337cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check for env, actor, loss, and collector\n",
    "print(\"=\" * 50)\n",
    "print(\"Testing Environment\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test environment reset and step\n",
    "test_td = serial_env.reset()\n",
    "print(f\"Reset output keys: {test_td.keys()}\")\n",
    "print(f\"Observation shape: {test_td['observation'].shape}\")\n",
    "print(f\"Action mask shape: {test_td['action_mask'].shape}\")\n",
    "\n",
    "# Take a random action\n",
    "test_td = serial_env.rand_step(test_td)\n",
    "print(f\"Step output keys: {test_td.keys()}\")\n",
    "print(f\"Reward: {test_td.get(\"next\", {}).get('reward', 'N/A')}\")\n",
    "print(f\"Done: {test_td['done']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Testing Actor\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test actor forward pass\n",
    "test_td = serial_env.reset()\n",
    "with torch.no_grad():\n",
    "    actor_output = actor(test_td)\n",
    "\n",
    "print(f\"Actor output keys: {actor_output.keys()}\")\n",
    "print(f\"Action shape: {actor_output['action'].shape}\")\n",
    "print(f\"Action value: {actor_output['action'].item()}\")\n",
    "\n",
    "# Test loss calculation with dummy data\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Testing Loss Function and Backpropagation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "actor_params_before = {name: p.clone() for name, p in actor.named_parameters()}\n",
    "# Giả sử biến mạng Q-value của bạn tên là qvalue_network hoặc critic\n",
    "# Nếu bạn dùng SAC/TD3 trong TorchRL, nó thường nằm trong loss_module hoặc là một module riêng\n",
    "# Hãy thay 'qvalue_network' bằng tên biến thực tế của bạn (ví dụ: loss_fn.qvalue_network_params hoặc critic)\n",
    "try:\n",
    "    # Ví dụ nếu bạn có biến 'qvalue_network'\n",
    "    qvalue_params_before = {name: p.clone() for name, p in qvalue_network.named_parameters()}\n",
    "    # qvalue_params_before = {name: p.clone() for name, p in loss_fn.qvalue_network_params.flatten_keys(\".\").to_dict().values()}\n",
    "except NameError:\n",
    "    print(\"⚠️ Could not find 'qvalue_network' variable to check. Skipping Q-net check.\")\n",
    "    qvalue_params_before = {name: p.clone() for name, p in loss_fn.qvalue_network_params}\n",
    "    qvalue_params_before = None\n",
    "\n",
    "test_td = serial_env.reset()\n",
    "test_td = serial_env.rand_step(test_td)\n",
    "test_batch = test_td.repeat(BATCH_SIZE).contiguous().to(DEVICE)  # Create batch\n",
    "loss_dict = loss_fn(test_batch)\n",
    "\n",
    "print(f\"Loss keys: {loss_dict.keys()}\")\n",
    "for key, value in loss_dict.items():\n",
    "    if isinstance(value, Tensor):\n",
    "        print(f\"{key}: {value.item():.4f}\")\n",
    "\n",
    "# Test loss back-propagation\n",
    "optimizer.zero_grad()\n",
    "loss = loss_dict['loss_actor'] + loss_dict['loss_alpha'] + loss_dict['loss_qvalue']\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "updater.step()\n",
    "\n",
    "print(\"Gradient check for Actor before step:\")\n",
    "total_norm = 0\n",
    "for p in actor.parameters():\n",
    "    if p.grad is not None:\n",
    "        total_norm += p.grad.data.norm(2).item()\n",
    "print(f\"  Actor grad norm: {total_norm:.6f}\")\n",
    "print(\"-\" * 30)\n",
    "check_params_changed(actor, \"Actor\", actor_params_before)\n",
    "\n",
    "print(\"Gradient check for Q-value network before step:\")\n",
    "total_norm = 0\n",
    "for p in qvalue_network.parameters():\n",
    "    if p.grad is not None:\n",
    "        total_norm += p.grad.data.norm(2).item()\n",
    "print(f\"  Q-Value Network grad norm: {total_norm:.6f}\")\n",
    "if qvalue_params_before:\n",
    "    check_params_changed(qvalue_network, \"Q-Value Network\", qvalue_params_before)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Testing Loss Backpropagation Completed\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test collector (collect a small batch)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Testing Collector\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "collector_iter = iter(collector)\n",
    "batch = next(collector_iter)\n",
    "print(f\"Collected batch keys: {batch.keys()}\")\n",
    "print(f\"Batch size: {batch.batch_size}\")\n",
    "print(f\"Number of frames: {len(batch)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"All components working! ✓\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test if actor selects invalid actions\n",
    "print(\"=\" * 50)\n",
    "print(\"Testing Actor Action Validity\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run multiple episodes and check for invalid actions\n",
    "n_test_episodes = 10\n",
    "invalid_actions_count = 0\n",
    "total_steps = 0\n",
    "\n",
    "for episode in range(n_test_episodes):\n",
    "    test_td = serial_env.reset()\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    \n",
    "    while not done and step_count < 100:  # Max 100 steps per episode\n",
    "        # Get action from actor\n",
    "        with torch.no_grad():\n",
    "            test_td = actor(test_td)\n",
    "        \n",
    "        # Extract action and action_mask\n",
    "        action = test_td['action'].item()\n",
    "        action_mask = test_td['action_mask']\n",
    "        \n",
    "        # Check if action is valid\n",
    "        is_valid = action_mask[..., action].item()\n",
    "        \n",
    "        if not is_valid:\n",
    "            invalid_actions_count += 1\n",
    "            print(f\"⚠️ Episode {episode}, Step {step_count}: Invalid action {action}\")\n",
    "            print(f\"   Action mask: {action_mask.nonzero(as_tuple=True)[0].tolist()}\")\n",
    "        \n",
    "        # Take step in environment\n",
    "        try:\n",
    "            test_td = serial_env.step(test_td)\n",
    "            done = test_td['done'].item()\n",
    "            total_steps += 1\n",
    "            step_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error at episode {episode}, step {step_count}: {e}\")\n",
    "            break\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Results:\")\n",
    "print(f\"Total episodes: {n_test_episodes}\")\n",
    "print(f\"Total steps: {total_steps}\")\n",
    "print(f\"Invalid actions: {invalid_actions_count}\")\n",
    "print(f\"Invalid action rate: {invalid_actions_count/total_steps*100:.2f}%\")\n",
    "\n",
    "if invalid_actions_count == 0:\n",
    "    print(\"✅ No invalid actions detected!\")\n",
    "else:\n",
    "    print(\"⚠️ Invalid actions detected! Check MaskedCategorical configuration.\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
