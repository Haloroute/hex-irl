{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d8893b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4262247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchrl\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torch import Tensor\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import (\n",
    "    Binary,\n",
    "    Bounded,\n",
    "    Categorical,\n",
    "    Composite,\n",
    "    LazyTensorStorage,\n",
    "    ReplayBuffer,\n",
    "    TensorSpec,\n",
    "    UnboundedContinuous\n",
    ")\n",
    "from torchrl.envs import EnvBase, SerialEnv\n",
    "from torchrl.envs.transforms import ActionMask, TransformedEnv\n",
    "from torchrl.modules import MaskedCategorical, ProbabilisticActor, TanhNormal\n",
    "from torchrl.objectives.sac import DiscreteSACLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b431a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 64 # Batch size for training\n",
    "LR = 1e-3 # Learning rate for the optimizer\n",
    "WEIGHT_DECAY = 1e-4 # Weight decay for the optimizer\n",
    "\n",
    "MAX_BOARD_SIZE = 4 # Maximum board size for the Hex game\n",
    "N_CHANNEL = 4 # Number of channels for the observation (Red, Blue, Current Player, Valid Board)\n",
    "BOARD_SIZE = 4 # Size of the Hex board (board_size x board_size)\n",
    "SWAP_RULE = True # Whether to use the swap rule in the Hex game\n",
    "\n",
    "BUFFER_SIZE = 10000 # Size of the replay buffer\n",
    "N_FRAMES_PER_BATCH = 1024 # Number of frames to store in the replay buffer per episode\n",
    "STORAGE_DEVICE = 'cpu' # Device for storing the replay buffer data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f97d6",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dfd701",
   "metadata": {},
   "source": [
    "## Base Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7bfea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HexEnv(EnvBase):\n",
    "    def __init__(self, \n",
    "                 board_size: int,\n",
    "                 max_board_size: int = MAX_BOARD_SIZE,\n",
    "                 swap_rule: bool = SWAP_RULE,\n",
    "                 device: torch.device = DEVICE,\n",
    "                #  batch_size: torch.Size = torch.Size()\n",
    "                ):\n",
    "\n",
    "        # Assertions\n",
    "        assert board_size >= 1, \"Board size must be greater than or equal to 1.\"\n",
    "        assert board_size <= max_board_size, \"Board size must be less than or equal to max Board size.\"\n",
    "\n",
    "        super().__init__(device=device, spec_locked=False)\n",
    "\n",
    "        # Parameters\n",
    "        self.board_size: int = board_size\n",
    "        self.max_board_size: int = max_board_size\n",
    "        self.n_channel: int = N_CHANNEL\n",
    "        self.swap_rule: bool = swap_rule\n",
    "        # self.device: torch.device = device\n",
    "        # self.batch_size: torch.Size = batch_size # No batching at all\n",
    "\n",
    "        # Create shape variables\n",
    "        self.board_shape: torch.Size = torch.Size(\n",
    "            (self.max_board_size, self.max_board_size)\n",
    "        ) # (max_board_size, max_board_size)\n",
    "\n",
    "        # Valid board mask\n",
    "        self.valid_board: Tensor = torch.zeros(\n",
    "            self.board_shape, \n",
    "            dtype=torch.bool, \n",
    "            device=self.device\n",
    "        ) # (max_board_size, max_board_size)\n",
    "        self.valid_board[:self.board_size, :self.board_size] = 1\n",
    "\n",
    "        # Create private spec variables\n",
    "        self.observation_spec = Composite({\n",
    "            \"observation\": Binary(\n",
    "                shape=self.board_shape + (self.n_channel,),\n",
    "                # (max_board_size, max_board_size, n_channel)\n",
    "                device=self.device,\n",
    "                dtype=torch.float32\n",
    "            ),\n",
    "            \"action_mask\": Binary(\n",
    "                shape=(self.max_board_size ** 2,),\n",
    "                # (max_board_size ** 2,)\n",
    "                device=self.device,\n",
    "                dtype=torch.bool\n",
    "            )\n",
    "        })\n",
    "        self.action_spec = Categorical(\n",
    "            n=self.max_board_size ** 2,\n",
    "            # Number of discrete actions for each side of the board\n",
    "            device=self.device,\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        self.reward_spec = UnboundedContinuous(\n",
    "            shape=(1,),\n",
    "            device=device,\n",
    "            dtype=torch.float32\n",
    "        ) # Reward for both players\n",
    "\n",
    "    def _reset(self, tensordict: TensorDict | None = None, **kwargs) -> TensorDict:\n",
    "        # Initialize a fresh board\n",
    "        board: Tensor = torch.full((self.max_board_size, self.max_board_size), -1, dtype=torch.long, device=self.device) # -1: empty, 0: player 0 (red), 1: player 1 (blue)\n",
    "        current_player: int = 0 # 0: player 0 (red), 1: player 1 (blue)\n",
    "        # valid_move: Tensor = self.valid_board.float() # All valid moves at the start\n",
    "        done: Tensor = torch.tensor(False, dtype=torch.bool, device=self.device) # Game not done\n",
    "        # reward: Tensor = torch.tensor([0.0], dtype=torch.float32, device=self.device) # No reward at the start\n",
    "\n",
    "        # Create fresh observation, mask, done, reward\n",
    "        fresh_action: Tensor = torch.tensor([0], dtype=torch.long, device=self.device) # Placeholder action\n",
    "        fresh_observation: Tensor = torch.zeros((self.max_board_size, self.max_board_size, self.n_channel), dtype=torch.float32, device=self.device) # (max_board_size, max_board_size, n_channel)\n",
    "        fresh_observation[..., 0] = (board == 0).float() # Red pieces channel\n",
    "        fresh_observation[..., 1] = (board == 1).float() # Blue pieces channel\n",
    "        fresh_observation[..., -2] = current_player # 0: player 0 (red), 1: player 1 (blue)\n",
    "        fresh_observation[..., -1] = self.valid_board.clone().float() # (max_board_size, max_board_size) Playable board mask\n",
    "        fresh_action_mask: Tensor = self.valid_board.clone().bool().flatten() # (max_board_size ** 2,) Valid move mask\n",
    "        fresh_done: Tensor = done # Not done\n",
    "        # fresh_reward: Tensor = reward # No reward at the start\n",
    "\n",
    "        # # Update action spec for the environment\n",
    "        # self.action_spec.update_mask(fresh_action_mask.flatten())\n",
    "\n",
    "        fresh_tensordict: TensorDict = TensorDict({\n",
    "            \"action\": fresh_action,\n",
    "            \"observation\": fresh_observation,\n",
    "            \"action_mask\": fresh_action_mask,\n",
    "            \"done\": fresh_done,\n",
    "            # \"reward\": fresh_reward\n",
    "        }, device=self.device)\n",
    "        # # Update tensordict\n",
    "        # if not isinstance(tensordict, TensorDict):\n",
    "        #     fresh_tensordict = TensorDict({\n",
    "        #         \"action\": fresh_action,\n",
    "        #         \"observation\": fresh_observation,\n",
    "        #         \"action_mask\": fresh_action_mask,\n",
    "        #         \"done\": fresh_done,\n",
    "        #         # \"reward\": fresh_reward\n",
    "        #     }, device=self.device)\n",
    "        # else:\n",
    "        #     fresh_tensordict: TensorDict = tensordict\n",
    "        #     fresh_tensordict.update({\n",
    "        #         \"action\": fresh_action,\n",
    "        #         \"observation\": fresh_observation,\n",
    "        #         \"action_mask\": fresh_action_mask,\n",
    "        #         \"done\": fresh_done,\n",
    "        #         # \"reward\": fresh_reward\n",
    "        #     })\n",
    "\n",
    "        return fresh_tensordict\n",
    "\n",
    "    def _step(self, tensordict: TensorDict, **kwargs) -> TensorDict:\n",
    "        # Extract action\n",
    "        action: Tensor = tensordict.get(\"action\").clone() # Scalar tensor representing the action\n",
    "        observation: Tensor = tensordict.get(\"observation\").clone() # (max_board_size, max_board_size, n_channel)\n",
    "        action_mask: Tensor = tensordict.get(\"action_mask\").clone() # (max_board_size ** 2,)\n",
    "        done: Tensor = tensordict.get(\"done\").clone() # Scalar tensor representing if the game is done\n",
    "        reward: Tensor = self.reward_spec.zero() # Initialize reward tensor # (1,)\n",
    "        # reward: Tensor = tensordict.get(\"reward\").clone() # (2,)\n",
    "\n",
    "        # Extract indexes of action from observation\n",
    "        index: int = int(action.item())\n",
    "        row, col = divmod(index, self.max_board_size) # Convert flat index to 2D coordinates\n",
    "\n",
    "        # Extract current state from observation\n",
    "        current_player: int = int(observation[0, 0, -2].item()) # 0: player 0 (red), 1: player 1 (blue)\n",
    "\n",
    "        # Check if this is a swap situation\n",
    "        is_first_move = (torch.sum(observation[..., 0:2]).item() == 0 and\n",
    "                         current_player == 0)  # Player 0's turn and no pieces placed yet\n",
    "        is_second_move = (torch.sum(observation[..., 0:2]).item() == 1 and\n",
    "                          current_player == 1)  # Player 1's turn and only one piece placed\n",
    "        is_swap_action = (self.swap_rule and\n",
    "                        is_second_move and\n",
    "                        observation[row, col, 0] == 1) # Player 1 selecting player 0's piece\n",
    "\n",
    "        # Validate action\n",
    "        is_valid = (\n",
    "            0 <= row < self.max_board_size and # Must be within board's max bounds\n",
    "            0 <= col < self.max_board_size and # Must be within board's max bounds\n",
    "            self.valid_board[row, col] and # Must be in valid board area\n",
    "            (action_mask[index] == 1 or is_swap_action)  # Must be empty to place a piece, or a valid swap action\n",
    "        )\n",
    "\n",
    "        # If action is not valid (only when action_spec mask is not working properly)\n",
    "        if not is_valid:\n",
    "            raise ValueError(f\"Invalid action {action.item()} at row={row}; col={col}; valid={self.valid_board[row, col]}, action_mask={action_mask[index]}.\")\n",
    "            # reward[self.current_player - 1] = -1.0 # Penalty for invalid move\n",
    "            # self.done = False # Continue the game even if the move is invalid\n",
    "            # new_observation, new_action_mask = tensordict.get(\"observation\"), tensordict.get(\"action_mask\") # Keep previous observation and action_mask\n",
    "        else:\n",
    "            # Update action_mask to prevent placing another piece here\n",
    "            action_mask[index] = 0 # Update action_mask to prevent placing another piece here\n",
    "\n",
    "            # Place piece or swap\n",
    "            if is_swap_action: # Swap the pieces\n",
    "                observation[..., 0], observation[..., 1] = observation[..., 1].clone(), observation[..., 0].clone()\n",
    "            else: # Place the piece on the board\n",
    "                observation[row, col, current_player] = 1.0 # Update observation for the current player\n",
    "\n",
    "            # Check for win condition (placeholder logic)\n",
    "            if self._check_done(observation, current_player):\n",
    "                reward: Tensor = torch.tensor([1.0 * (1 - current_player) - 1.0 * current_player], dtype=torch.float32, device=self.device) # Single reward for the current player (+1 if player 0 wins, -1 if player 1 wins)\n",
    "                done = torch.tensor(True, dtype=torch.bool) # Game done\n",
    "            else:\n",
    "                reward: Tensor = torch.tensor([0.0], dtype=torch.float32, device=self.device) # Initialize reward\n",
    "                done = torch.tensor(False, dtype=torch.bool) # Game not done\n",
    "\n",
    "                # Switch player\n",
    "                current_player = 1 - current_player # Switch between 0 and 1\n",
    "\n",
    "            # Update observation, action_mask\n",
    "            new_observation: Tensor = torch.zeros((self.max_board_size, self.max_board_size, self.n_channel), dtype=torch.float, device=self.device) # (max_board_size, max_board_size, n_channel)\n",
    "            new_observation[..., 0] = observation[..., 0] # Red pieces channel\n",
    "            new_observation[..., 1] = observation[..., 1] # Blue pieces channel\n",
    "            new_observation[..., -2] = float(current_player) # Current player channel\n",
    "            new_observation[..., -1] = observation[..., -1] # (max_board_size, max_board_size) Playable board action_mask (doesn't change)\n",
    "            new_action_mask: Tensor = action_mask.bool() # Valid move action_mask\n",
    "\n",
    "        # Create done, reward tensors\n",
    "        new_action: Tensor = action\n",
    "        new_done: Tensor = done\n",
    "        new_reward: Tensor = reward\n",
    "\n",
    "        # # Update action spec for the environment\n",
    "        # if is_first_move and self.swap_rule:\n",
    "        #     # Allow swap action if it's the first move and swap rule is enabled\n",
    "        #     swap_action_mask = new_action_mask.clone()\n",
    "        #     swap_action_mask[index] = 1 # Allow the swap action\n",
    "        #     self.action_spec.update_action_mask(swap_action_mask.flatten())\n",
    "        # else:\n",
    "        #     # Update action spec for the environment\n",
    "        #     self.action_spec.update_action_mask(new_action_mask.flatten())\n",
    "\n",
    "        # Update tensordict\n",
    "        new_tensordict = TensorDict({\n",
    "            \"action\": new_action,\n",
    "            \"observation\": new_observation,\n",
    "            \"action_mask\": new_action_mask,\n",
    "            \"done\": new_done,\n",
    "            \"reward\": new_reward\n",
    "        }, device=self.device)\n",
    "\n",
    "        return new_tensordict\n",
    "\n",
    "    def _check_done(self, observation: Tensor, current_player: int) -> bool:\n",
    "        def dfs(board, start_positions, target_condition, directions):\n",
    "            visited = torch.zeros((self.board_size, self.board_size), dtype=torch.bool)\n",
    "            for start in start_positions:\n",
    "                if board[start] == 1 and not visited[start]:\n",
    "                    stack = [start]\n",
    "                    visited[start] = True\n",
    "                    while stack:\n",
    "                        r, c = stack.pop()\n",
    "                        if target_condition(r, c):\n",
    "                            return True\n",
    "                        for dr, dc in directions:\n",
    "                            nr, nc = r + dr, c + dc\n",
    "                            if 0 <= nr < self.board_size and 0 <= nc < self.board_size and board[nr, nc] == 1 and not visited[nr, nc]:\n",
    "                                visited[nr, nc] = True\n",
    "                                stack.append((nr, nc))\n",
    "            return False\n",
    "\n",
    "        directions = [(-1,0), (1,0), (0,-1), (0,1), (1,-1), (-1,1)] # 6 possible directions in a hex grid\n",
    "        board_state = observation[:self.board_size, :self.board_size, :]\n",
    "        # Use DFS to check if player 0 (red) has connected top to bottom\n",
    "        if current_player == 0:\n",
    "            board = board_state[..., 0] # Shape (board_size, board_size) # Player 0 pieces\n",
    "            start_positions = [(0, col) for col in range(self.board_size)]\n",
    "            target_condition = lambda r, c: r == self.board_size - 1\n",
    "            if dfs(board, start_positions, target_condition, directions):\n",
    "                return True\n",
    "\n",
    "        # Use DFS to check if player 1 (blue) has connected left to right\n",
    "        else:\n",
    "            board = board_state[..., 1] # Shape (board_size, board_size) # Player 1 pieces\n",
    "            start_positions = [(row, 0) for row in range(self.board_size)]\n",
    "            target_condition = lambda r, c: c == self.board_size - 1\n",
    "            if dfs(board, start_positions, target_condition, directions):\n",
    "                return True\n",
    "\n",
    "        return False # No winner yet\n",
    "\n",
    "    def _set_seed(self, seed: int) -> None:\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0906b725",
   "metadata": {},
   "source": [
    "## Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ec1a9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action_mask: Tensor(shape=torch.Size([1, 16]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([1, 4, 4, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([1]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_hex_env = lambda: HexEnv(board_size=BOARD_SIZE, max_board_size=MAX_BOARD_SIZE, device=STORAGE_DEVICE)\n",
    "serial_env = TransformedEnv(\n",
    "    SerialEnv(\n",
    "        num_workers=1,\n",
    "        create_env_fn=create_hex_env\n",
    "    ),\n",
    "    ActionMask()\n",
    ")\n",
    "# serial_env = TransformedEnv(\n",
    "#     HexEnv(\n",
    "#         board_size=BOARD_SIZE,\n",
    "#         max_board_size=MAX_BOARD_SIZE,\n",
    "#         device=STORAGE_DEVICE\n",
    "#     ),\n",
    "#     ActionMask()\n",
    "# )\n",
    "\n",
    "r = serial_env.rollout(100)\n",
    "r[\"action\"].to(dtype=torch.int)\n",
    "serial_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f822f2b6",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4739e97",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f2bb610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HexConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super(HexConv2d, self).__init__()\n",
    "        assert kernel_size % 2 == 1 and kernel_size > 0, \"kernel_size must be odd and positive.\"\n",
    "        stride, padding = 1, kernel_size // 2  # To maintain spatial dimensions\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.mask = nn.Parameter(self._create_hex_mask(kernel_size), requires_grad=False) # (k, k), requires_grad=False to keep it fixed\n",
    "        with torch.no_grad():\n",
    "            self.conv.weight.mul_(self.mask)  # Apply mask to weights\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_hex_mask(kernel_size: int) -> Tensor:\n",
    "        assert kernel_size % 2 == 1 and kernel_size > 0, \"kernel_size must be odd and positive.\"\n",
    "\n",
    "        mask = torch.zeros((kernel_size, kernel_size), dtype=torch.float32)\n",
    "        center = kernel_size // 2\n",
    "\n",
    "        for r in range(kernel_size): # Row index\n",
    "            for c in range(kernel_size): # Column index\n",
    "                # Using axial distance for a vertically oriented hex grid\n",
    "                # mapped to an offset coordinate system in the kernel.\n",
    "                # (r, c) are kernel indices, (dr, dc) are relative to center.\n",
    "                dr, dc = r - center, c - center\n",
    "                chebyshev_distance = max(abs(dr), abs(dc), abs(dr + dc))\n",
    "\n",
    "                if chebyshev_distance <= center: # Inside or on the hexagon\n",
    "                    mask[r, c] = 1.0\n",
    "\n",
    "        return mask # (kernel_size, kernel_size)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Assuming x is of shape (batch_size, channels, height, width)\n",
    "        # Apply convolution\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SkipConnection(nn.Module):\n",
    "    def __init__(self, adjust_input: nn.Module, original_input: nn.Module = nn.Identity()):\n",
    "        super(SkipConnection, self).__init__()\n",
    "        self.adjust_input = adjust_input\n",
    "        self.original_input = original_input\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.adjust_input(x) + self.original_input(x)\n",
    "\n",
    "\n",
    "class HexModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 conv_layers: list[tuple[int, int]],\n",
    "                 n_encoder_layers: int,\n",
    "                 d_input: int,\n",
    "                 n_heads: int = 8,\n",
    "                 d_ff: int = 2048,\n",
    "                 dropout: float = 0.1,\n",
    "                 output_flatten: bool = True):\n",
    "        \"\"\"Args:\n",
    "            conv_layers: List of tuples (out_channels, kernel_size) for each conv layer.\n",
    "                Note that, in_channels is inferred from the previous layer's out_channels (d_input for the first layer).\n",
    "            n_encoder_layers: Number of transformer encoder layers.\n",
    "            d_input: Dimension of input features to the transformer.\n",
    "            n_heads: Number of attention heads in the transformer.\n",
    "            d_ff: Dimension of the feedforward network in the transformer.\n",
    "            dropout: Dropout rate.\n",
    "        \"\"\"\n",
    "        super(HexModel, self).__init__()\n",
    "        self.output_flatten = output_flatten\n",
    "        self.d_encoder: int = conv_layers[-1][0] # Last conv layer's out_channels as d_model\n",
    "        self.conv = nn.Sequential(*[\n",
    "            SkipConnection(\n",
    "                nn.Sequential(\n",
    "                    HexConv2d(conv_layers[i-1][0] if i > 0 else d_input, conv_layers[i][0], conv_layers[i][1]),\n",
    "                    nn.BatchNorm2d(conv_layers[i][0]),\n",
    "                    nn.PReLU(),\n",
    "                    HexConv2d(conv_layers[i][0], conv_layers[i][0], conv_layers[i][1]),\n",
    "                    nn.BatchNorm2d(conv_layers[i][0]),\n",
    "                ),\n",
    "                nn.Identity() if conv_layers[i][0] == conv_layers[i-1][0] # Skip connection (identity)\n",
    "                else HexConv2d(conv_layers[i-1][0] if i > 0 else d_input, conv_layers[i][0], 1) # Combine with Conv2d (kernel_size = 1) for channel adjustment\n",
    "            )\n",
    "            for i in range(len(conv_layers))\n",
    "        ])\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=self.d_encoder,\n",
    "                nhead=n_heads,\n",
    "                dim_feedforward=d_ff,\n",
    "                dropout=dropout,\n",
    "                activation='gelu',\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=n_encoder_layers\n",
    "        )\n",
    "        self.projection = nn.Linear(self.d_encoder, 1) # Đầu ra cho Actor (logits)/Critic (Q-value)\n",
    "\n",
    "    def forward(self, x: Tensor, tensordict: TensorDict | None = None) -> Tensor:\n",
    "        \"\"\"Args:\n",
    "            x: Input tensor of shape (N, H, W, C) where\n",
    "               N = batch size, H = height, W = width, C = channels (d_input).\n",
    "            Returns: Tensor of shape (N, H, W) with Q-values for each position.\n",
    "        \"\"\"\n",
    "        # Reshape input to (N, C, H, W) for Conv2d\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(0)  # Add batch dimension if missing\n",
    "        elif len(x.shape) != 4:\n",
    "            raise ValueError(f\"Input tensor x must have shape (N, H, W, C) or (H, W, C), but got {x.shape}.\")\n",
    "\n",
    "        batch_size, height, width = x.size(0), x.size(1), x.size(2)\n",
    "        x = x.permute(0, 3, 1, 2).contiguous() # (N, C, H, W)\n",
    "        x = self.conv(x) # (N, d_encoder, H, W)\n",
    "        # Reshape to (N, H*W, d_encoder)\n",
    "        x = x.view(batch_size, -1, self.d_encoder) # (N, H*W, d_encoder)\n",
    "        x = self.encoder(x) # (N, H*W, d_encoder)\n",
    "\n",
    "        # Create outputs for Actor/Critic\n",
    "        x = self.projection(x) # (N, H*W, 1)\n",
    "        if self.output_flatten:\n",
    "            return x.view(batch_size, -1) # (N, H*W)\n",
    "        else:\n",
    "            return x.view(batch_size, height, width) # (N, H, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f307f84e",
   "metadata": {},
   "source": [
    "## Policy Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "731454aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorWrapper(nn.Module):\n",
    "    \"\"\"Bọc TransformerQL_AC, chỉ trả về 'logits'.\"\"\"\n",
    "    def __init__(self, model: HexModel):\n",
    "        super().__init__()\n",
    "        self.model = model # Tham chiếu đến model chung\n",
    "\n",
    "    def forward(self, observation: Tensor, action_mask: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        action_mask = action_mask.view(observation.shape[0], -1) # (N, H*W)\n",
    "        \n",
    "        # Chạy model chung, chỉ lấy đầu ra đầu tiên\n",
    "        logits = self.model(observation) # logits shape (N, H*W)\n",
    "        \n",
    "        # logits[~action_mask] = -torch.inf # Áp dụng mask\n",
    "        return logits, action_mask\n",
    "\n",
    "\n",
    "class CriticWrapper(nn.Module):\n",
    "    \"\"\"Bọc HexModel, chỉ trả về 'action_value'.\"\"\"\n",
    "    def __init__(self, model: HexModel):\n",
    "        super().__init__()\n",
    "        self.model = model # Tham chiếu đến CÙNG model chung\n",
    "\n",
    "    def forward(self, observation: Tensor, action_mask: Tensor) -> Tensor:\n",
    "        # action_mask = action_mask.view(observation.shape[0], -1)\n",
    "        \n",
    "        # Chạy model chung, chỉ lấy đầu ra thứ hai\n",
    "        q_values = self.model(observation) # q_values shape (N, H*W)\n",
    "        \n",
    "        # q_values[~action_mask] = -torch.inf # Áp dụng mask\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1718210c",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5986ee8",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40981d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tạo một model chung\n",
    "model = HexModel(\n",
    "    conv_layers=[(32, 3), (64, 3)],\n",
    "    n_encoder_layers=1,\n",
    "    d_input=N_CHANNEL,\n",
    "    n_heads=2,\n",
    "    d_ff=1024,\n",
    "    dropout=0.1,\n",
    "    output_flatten=True\n",
    ").to(DEVICE) # A sample model\n",
    "\n",
    "# 2. Tạo hai wrapper và policy\n",
    "actor_network = TensorDictModule(\n",
    "    ActorWrapper(model),\n",
    "    in_keys=[\"observation\", \"action_mask\"],\n",
    "    out_keys=[\"logits\", \"mask\"]\n",
    ")\n",
    "qvalue_network = TensorDictModule(\n",
    "    CriticWrapper(model),\n",
    "    in_keys=[\"observation\", \"action_mask\"],\n",
    "    out_keys=[\"action_value\"]\n",
    ")\n",
    "actor = ProbabilisticActor(\n",
    "    actor_network,\n",
    "    in_keys=[\"logits\", \"mask\"],\n",
    "    spec=serial_env.action_spec,\n",
    "    distribution_class=MaskedCategorical\n",
    ")\n",
    "\n",
    "# 3. Tạo loss_fn (num_qvalue_nets=1 vì wrapper chỉ trả về 1 Q-value)\n",
    "loss_fn = DiscreteSACLoss(\n",
    "    actor_network=actor,\n",
    "    qvalue_network=qvalue_network,\n",
    "    action_space=serial_env.action_spec,\n",
    "    num_actions=serial_env.action_spec.n\n",
    ").to(DEVICE)\n",
    "\n",
    "# 4. Thiết lập optimizer, replay buffer, và các thành phần khác như bình thường\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": loss_fn.parameters()}\n",
    "    ],\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(BUFFER_SIZE, device=STORAGE_DEVICE),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "collector = SyncDataCollector(\n",
    "    create_env_fn=serial_env,\n",
    "    policy=actor,\n",
    "    frames_per_batch=N_FRAMES_PER_BATCH,\n",
    "    total_frames=-1, # Vô hạn\n",
    "    device=DEVICE,\n",
    "    storing_device=STORAGE_DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b31408e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing Environment\n",
      "==================================================\n",
      "Reset output keys: _StringKeys(dict_keys(['action_mask', 'done', 'observation', 'terminated']))\n",
      "Observation shape: torch.Size([1, 4, 4, 4])\n",
      "Action mask shape: torch.Size([1, 16])\n",
      "Step output keys: _StringKeys(dict_keys(['action_mask', 'done', 'observation', 'terminated', 'action', 'next']))\n",
      "Reward: tensor([[0.]], device='cuda:0')\n",
      "Done: tensor([[False]], device='cuda:0')\n",
      "\n",
      "==================================================\n",
      "Testing Actor\n",
      "==================================================\n",
      "Actor output keys: _StringKeys(dict_keys(['action_mask', 'done', 'observation', 'terminated', 'logits', 'mask', 'action']))\n",
      "Action shape: torch.Size([1])\n",
      "Action value: 11\n",
      "\n",
      "==================================================\n",
      "Testing Loss Function\n",
      "==================================================\n",
      "Loss keys: _StringKeys(dict_keys(['loss_actor', 'loss_qvalue', 'loss_alpha', 'alpha', 'entropy']))\n",
      "loss_actor: -2.3900\n",
      "loss_qvalue: 4.5871\n",
      "loss_alpha: 0.0000\n",
      "alpha: 1.0000\n",
      "entropy: 2.7068\n",
      "\n",
      "==================================================\n",
      "Testing Collector\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\hex\\Lib\\site-packages\\torchrl\\objectives\\common.py:40: UserWarning: No target network updater has been associated with this loss module, but target parameters have been found. While this is supported, it is expected that the target network updates will be manually performed. You can deactivate this warning by turning the RL_WARNINGS env variable to False.\n",
      "  warnings.warn(\n",
      "d:\\miniconda3\\envs\\hex\\Lib\\site-packages\\torchrl\\objectives\\common.py:456: UserWarning: No target network updater has been associated with this loss module, but target parameters have been found. While this is supported, it is expected that the target network updates will be manually performed. You can deactivate this warning by turning the RL_WARNINGS env variable to False.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected batch keys: _StringKeys(dict_keys(['action', 'observation', 'action_mask', 'done', 'terminated', 'next', 'collector', 'mask', 'logits']))\n",
      "Batch size: torch.Size([1, 1024])\n",
      "Number of frames: 1\n",
      "\n",
      "==================================================\n",
      "All components working! ✓\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check for env, actor, loss, and collector\n",
    "print(\"=\" * 50)\n",
    "print(\"Testing Environment\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test environment reset and step\n",
    "test_td = serial_env.reset()\n",
    "print(f\"Reset output keys: {test_td.keys()}\")\n",
    "print(f\"Observation shape: {test_td['observation'].shape}\")\n",
    "print(f\"Action mask shape: {test_td['action_mask'].shape}\")\n",
    "\n",
    "# Take a random action\n",
    "test_td = serial_env.rand_step(test_td)\n",
    "print(f\"Step output keys: {test_td.keys()}\")\n",
    "print(f\"Reward: {test_td.get(\"next\", {}).get('reward', 'N/A')}\")\n",
    "print(f\"Done: {test_td['done']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Testing Actor\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test actor forward pass\n",
    "test_td = serial_env.reset()\n",
    "with torch.no_grad():\n",
    "    actor_output = actor(test_td)\n",
    "\n",
    "print(f\"Actor output keys: {actor_output.keys()}\")\n",
    "print(f\"Action shape: {actor_output['action'].shape}\")\n",
    "print(f\"Action value: {actor_output['action'].item()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Testing Loss Function\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test loss calculation with dummy data\n",
    "test_td = serial_env.reset()\n",
    "test_td = serial_env.rand_step(test_td)\n",
    "test_batch = test_td.expand(BATCH_SIZE).to(DEVICE)  # Create batch\n",
    "with torch.no_grad():\n",
    "    loss_dict = loss_fn(test_batch)\n",
    "\n",
    "print(f\"Loss keys: {loss_dict.keys()}\")\n",
    "for key, value in loss_dict.items():\n",
    "    if isinstance(value, Tensor):\n",
    "        print(f\"{key}: {value.item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Testing Collector\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test collector (collect a small batch)\n",
    "collector_iter = iter(collector)\n",
    "batch = next(collector_iter)\n",
    "print(f\"Collected batch keys: {batch.keys()}\")\n",
    "print(f\"Batch size: {batch.batch_size}\")\n",
    "print(f\"Number of frames: {len(batch)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"All components working! ✓\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9c9cf1",
   "metadata": {},
   "source": [
    "## Go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d31795f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing Actor Action Validity\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Results:\n",
      "Total episodes: 10\n",
      "Total steps: 1000\n",
      "Invalid actions: 0\n",
      "Invalid action rate: 0.00%\n",
      "✅ No invalid actions detected!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Test if actor selects invalid actions\n",
    "print(\"=\" * 50)\n",
    "print(\"Testing Actor Action Validity\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run multiple episodes and check for invalid actions\n",
    "n_test_episodes = 10\n",
    "invalid_actions_count = 0\n",
    "total_steps = 0\n",
    "\n",
    "for episode in range(n_test_episodes):\n",
    "    test_td = serial_env.reset()\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    \n",
    "    while not done and step_count < 100:  # Max 100 steps per episode\n",
    "        # Get action from actor\n",
    "        with torch.no_grad():\n",
    "            test_td = actor(test_td)\n",
    "        \n",
    "        # Extract action and action_mask\n",
    "        action = test_td['action'].item()\n",
    "        action_mask = test_td['action_mask']\n",
    "        \n",
    "        # Check if action is valid\n",
    "        is_valid = action_mask[..., action].item()\n",
    "        \n",
    "        if not is_valid:\n",
    "            invalid_actions_count += 1\n",
    "            print(f\"⚠️ Episode {episode}, Step {step_count}: Invalid action {action}\")\n",
    "            print(f\"   Action mask: {action_mask.nonzero(as_tuple=True)[0].tolist()}\")\n",
    "        \n",
    "        # Take step in environment\n",
    "        try:\n",
    "            test_td = serial_env.step(test_td)\n",
    "            done = test_td['done'].item()\n",
    "            total_steps += 1\n",
    "            step_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error at episode {episode}, step {step_count}: {e}\")\n",
    "            break\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Results:\")\n",
    "print(f\"Total episodes: {n_test_episodes}\")\n",
    "print(f\"Total steps: {total_steps}\")\n",
    "print(f\"Invalid actions: {invalid_actions_count}\")\n",
    "print(f\"Invalid action rate: {invalid_actions_count/total_steps*100:.2f}%\")\n",
    "\n",
    "if invalid_actions_count == 0:\n",
    "    print(\"✅ No invalid actions detected!\")\n",
    "else:\n",
    "    print(\"⚠️ Invalid actions detected! Check MaskedCategorical configuration.\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
